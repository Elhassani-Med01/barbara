<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projet IA Chatbot Pour l'ENSAM</title>
    <style>
        body {
            font-family: 'Verdana', sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f9f9f9;
        }
        h1, h2, h3 {
            color: #333;
        }
        pre {
            background: #eee;
            padding: 10px;
            border: 1px solid #ccc;
            overflow-x: auto;
        }
        code {
            color: #0056b3;
            background-color: #f1f8ff;
            padding: 2px 4px;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <h1>Projet IA Chatbot Pour l'ENSAM</h1>

    <h2>Co-fondateurs</h2>
    <p>Elhassani Mohamed et Romaissae Belhadj</p>

    <h2>Introduction</h2>
    <p>Ce projet utilise <code>Streamlit</code> et <code>LangChain</code> pour créer un chatbot interactif spécialisé dans les questions relatives à l'ENSAM. Ce document décrit chaque partie du code, en expliquant son rôle dans l'application.</p>

    <h2>Code détaillé et explications</h2>

    <h3>1. Chargement des bibliothèques</h3>
    <pre><code>import os
import streamlit as st
from langchain_community.llms import Ollama
from dotenv import load_dotenv
from langchain_community.embeddings import OllamaEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA</code></pre>
    <p>Ces bibliothèques sont utilisées pour :</p>
    <ul>
        <li><code>os</code>: Gérer le système de fichiers.</li>
        <li><code>streamlit</code>: Créer une interface utilisateur interactive.</li>
        <li><code>LangChain</code>: Gérer les modèles, les embeddings et la recherche de documents.</li>
        <li><code>dotenv</code>: Charger les variables d'environnement.</li>
    </ul>

    <h3>2. Chargement des variables d'environnement</h3>
    <pre><code>load_dotenv()</code></pre>
    <p>Permet de charger les variables sensibles comme les clés API à partir d'un fichier <code>.env</code>.</p>

    <h3>3. Initialisation des modèles LLM et d'embedding</h3>
    <pre><code>llm = Ollama(model="mistral", base_url="http://127.0.0.1:11434")
embed_model = OllamaEmbeddings(model="mistral", base_url="http://127.0.0.1:11434")</code></pre>
    <p>Le modèle <code>mistral</code> est utilisé pour générer des réponses et créer des embeddings pour le texte.</p>

    <h3>4. Chargement du fichier texte</h3>
    <pre><code>file_path = 'C:/Users/nelba/Desktop/Chatbot/Chatbot/New.txt'

with open(file_path, 'r', encoding='utf-8') as file:
    file_content = file.read()</code></pre>
    <p>Lit le contenu du fichier texte contenant les données de référence pour répondre aux questions.</p>

    <h3>5. Division du texte en morceaux</h3>
    <pre><code>text_splitter = RecursiveCharacterTextSplitter(chunk_size=1350, chunk_overlap=110)
chunks = text_splitter.split_text(file_content)</code></pre>
    <p>Découpe le texte en morceaux pour faciliter le traitement et la recherche.</p>

    <h3>6. Gestion de la base de vecteurs</h3>
    <pre><code>vector_store_path='C:/Users/nelba/Desktop/Chatbot/Chatbot/VDB'
if not os.path.exists(vector_store_path):
    os.makedirs(vector_store_path)</code></pre>
    <p>Crée un dossier pour stocker la base de vecteurs si elle n'existe pas déjà.</p>

    <h3>7. Chargement ou création de la base de vecteurs</h3>
    <pre><code>if os.listdir(vector_store_path):
    vector_store = Chroma(persist_directory=vector_store_path, embedding_function=embed_model)
else:
    vector_store = Chroma.from_texts(chunks, embed_model, persist_directory=vector_store_path)
    vector_store.persist()</code></pre>
    <p>Charge une base existante ou en crée une nouvelle à partir des morceaux de texte.</p>

    <h3>8. Initialisation du retrieveur et de la chaîne QA</h3>
    <pre><code>retriever = vector_store.as_retriever()
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)</code></pre>
    <p>Crée un retrieveur pour interroger la base de vecteurs et une chaîne pour générer des réponses.</p>

    <h3>9. Interface utilisateur Streamlit</h3>
    <pre><code>st.title("Application Interactive - Questions sur ENSAM")
user_question = st.text_input("Votre question :", placeholder="Posez une question ici...")
if st.button("Obtenir la réponse"):
    ...</code></pre>
    <p>Permet à l'utilisateur de poser une question et de recevoir une réponse via l'interface.</p>

    <h3>10. Gestion des réponses</h3>
    <pre><code>if user_question.strip():
    prompt = f"""Tu es un assistant virtuel ...
    response = qa_chain({"query": prompt.strip()})
    result = response.get("result", "").strip()
    ...</code></pre>
    <p>Construit un prompt, exécute la chaîne QA et affiche la réponse ainsi que les sources, si disponibles.</p>

    <h2>Conclusion</h2>
    <p>Ce projet intègre plusieurs technologies pour créer un chatbot performant. Chaque composant du code joue un rôle clé pour garantir que les questions des utilisateurs reçoivent des réponses précises et basées sur les données fournies.</p>
</body>
</html>
